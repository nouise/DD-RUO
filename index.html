<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Joint Rate-Utility Optimization for Dataset Distillation</title>
    <!-- MathJax for LaTeX formula support -->
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>
        window.MathJax = {
            tex: {
                inlineMath: [['$', '$'], ['\\(', '\\)']],
                displayMath: [['$$', '$$'], ['\\[', '\\]']]
            }
        };
    </script>
    <link rel="stylesheet" href="data/style.css">
</head>
<body>
    <!-- Fixed Navigation Bar -->
    <nav class="navbar" id="navbar">
        <div class="nav-container">
            <div class="nav-title">Meta-Hitsz</div>
            <ul class="nav-links">
                <li><a href="#abstract">Abstract</a></li>
                <li><a href="#introduction">Introduction</a></li>
                <li><a href="#method">Methodology</a></li>
                <li><a href="#experiments">Experiments</a></li>
                <li><a href="#conclusion">Conclusion</a></li>
                <li><a href="#references">References</a></li>
            </ul>
            <button class="theme-toggle" id="themeToggle" title="Toggle theme">
                üåô
            </button>
        </div>
    </nav>
    <!-- GitHub Corner -->
    <a href="#" class="github-corner" aria-label="View source on GitHub" style="display: none;">
        <img src="data/img/github-corner.svg" alt="GitHub Corner" width="80" height="80">
    </a>
    <!-- Main Content -->
    <main class="main-content">
        <!-- Paper Header -->
        <header class="paper-header">
            <h1 class="paper-title">Dataset Distillation as Data Compression: A Rate-Utility Perspective</h1>
            <div class="authors">
                <a class="author" href="https://scholar.google.com/citations?user=lKD67zcAAAAJ&hl=en" title="younebao@cityu.edu.hk" target="_blank">Youneng Bao<sup>1,3*</sup></a>
                <a class="author" href="https://openreview.net/profile?id=~Yiping_Liu4" title="yipingliu@stu.hit.edu.cn" target="_blank">Yiping Liu<sup>3*</sup></a>
                <a class="author" href="http://chenzhuo.info/" title="chenzhuo.zoom@gmail.com" target="_blank">Zhuo Chen<sup>2</sup></a>
                <a class="author" href="https://scholar.google.com/citations?user=aOEk854AAAAJ&hl=zh-CN" title="liangys@hit.edu.cn" target="_blank">Yongsheng Liang<sup>1</sup></a>
                <a class="author" href="https://faculty.hitsz.edu.cn/limu" title="limu2022@hit.edu.cn" target="_blank">Mu Li<sup>1‚úâ</sup></a>
                <a class="author" href="https://kedema.org/" title="kede.ma@cityu.edu.hk" target="_blank">Kede Ma<sup>3‚úâ</sup></a>
            </div>
            <div class="affiliations">
                <p><sup>1</sup>Harbin Institute of Technology, Shenzhen</p>
                <p><sup>2</sup>Peng Cheng Laboratory</p>
                <p><sup>3</sup>City University of Hong Kong</p>
            </div>
            <div class="paper-links">
                <a href="#" class="paper-link">üìÑ arXiv</a>
                <a href="#" class="paper-link">üèõÔ∏è ICCV 2025</a>
                <a href="#" class="paper-link">üíª Code</a>
                <a href="#" class="paper-link">üìä Dataset</a>
            </div>
        </header>
        <!-- Abstract Section -->
        <section class="section" id="abstract">
            <h2 class="section-title">Abstract</h2>
            <div class="section-content">
                <p>Driven by the 'scale-is-everything' paradigm, modern machine learning increasingly demands ever-larger datasets and models, yielding prohibitive computational and storage requirements. Dataset distillation mitigates this by compressing a full dataset into a small set of synthetic samples, preserving its original utility. Yet, existing methods either optimize performance under a fixed storage budget or remove sample redundancy, without jointly balancing both objectives, thus precluding Pareto-optimal trade-offs.</p>
                <p>In this work, we propose a joint Rate-Utility Optimization (RUO) method for dataset distillation. Specifically, we parameterize synthetic samples as optimizable latent codes decoded by lightweight networks. Such a hybrid parameterization subsumes all prior explicit and implicit representations for synthetic datasets. We estimate the entropy of quantized latents as the rate measure and plug any existing distillation loss as the utility measure, trading them off via a Lagrange multiplier.</p>
                <p>To enable fair, cross-method comparisons, we introduce bits per class (bpc), a precise storage metric that accounts for sample, label, and decoder costs. On CIFAR-10, CIFAR-100, and ImageNet-128, our method achieves up to 170√ó greater compression than standard distillation at comparable accuracy. Across diverse bpc budgets, distillation losses, and backbone architectures, our approach consistently establishes new state-of-the-art rate-utility trade-offs.</p>
            </div>
        </section>
        <!-- Introduction Section -->
        <section class="section" id="introduction">
            <h2 class="section-title">1. Introduction</h2>
            <div class="section-content">
                <p>The rapid advances in machine learning have been driven by the 'scale-is-everything' paradigm, in which ever-larger models trained on ever-more extensive datasets yield progressively better performance across various computational prediction tasks. However, this scaling trend incurs steep computational, storage, and environmental costs, posing a significant barrier to sustainable and accessible research and deployment.</p>
                <div class="figure">
                    <img src="data/img/imgeNet_nette.png" alt="Rate-utility curves">
                    <figcaption class="figure-caption">Figure 1. The Rate-utility curves of different dataset distillation methods on ImageNet-subset Nette. Our method achieves a better trade-off between rate and utility compared to existing methods. BPC is defined as the bits per class, directly measuring the storage cost of the distilled dataset.</figcaption>
                </div>
                <p>Dataset distillation (DD) addresses these challenges by learning a compact, synthetic dataset that enables rapid model prototyping, accelerated model training, and efficient hyperparameter tuning, while lowering compute, storage, and energy requirements. At the core of DD lies a trade-off between rate (i.e., the storage footprint of the synthetic dataset) and utility (i.e., the performance of models trained on distilled data).</p>
                <p>Despite impressive progress, existing DD methods optimize rate and utility separately, precluding Pareto-optimal trade-offs. Inspired by end-to-end rate-distortion optimization of learned image compression, we describe a joint rate-utility optimization method for DD. Our approach parametrizes synthetic samples as multiscale grids of learnable latent codes, decoded by small learnable networks, which generalizes all existing explicit and implicit distilled-dataset representations.</p>
                <p><strong>Key Contributions:</strong></p>
                <ul>
                    <li>A joint rate-utility optimization method for DD that retains full compatibility with existing distillation losses</li>
                    <li>A hybrid parameterization of synthetic samples that subsumes prior synthetic dataset representations</li>
                    <li>A bpc metric for enabling standardized and fair rate-utility comparisons across various DD methods</li>
                    <li>An experimental demonstration on CIFAR-10, CIFAR-100, and ImageNet-128 that our method consistently establishes new state-of-the-art rate-utility trade-offs</li>
                </ul>
            </div>
        </section>
        <!-- Related Work Section -->
        <section class="section" id="related-work">
            <h2 class="section-title">2. Related Work</h2>
            <div class="section-content">
                <p>In this section, we review prior efforts in DD and data compression. Our method bridges the conceptual and computational gap between these two fields, allowing principled exploration of Pareto-optimal trade-offs between rate and utility in DD.</p>
                <h3 style="color: var(--primary-color); margin: 1.5rem 0 1rem 0;">2.1. Dataset Distillation</h3>
                <p>Wang et al. first formulated DD as a bilevel optimization problem, which generalizes coreset selection. The reliance of BPTT over the entire inner-loop trajectory, incurring prohibitive computation, high memory usage, and unstable meta-gradients. To reduce this burden, several meta-gradient estimation strategies have been proposed.</p>
                <p>Recent efforts have been put to address the scalability of DD methods to large, high-resolution datasets, typically via the decoupling trick in bilevel optimization. Close to our work is the so-called factorized DD, which factorizes the synthetic dataset with a combination of codes and decoders, in an attempt to improve compression efficiency.</p>
                <h3 style="color: var(--primary-color); margin: 1.5rem 0 1rem 0;">2.2. Data Compression</h3>
                <p>Data compression is broadly categorized into lossless and lossy compression paradigms. Neural compression, pioneered by recent works, employs neural networks to directly learn optimal transform strategies. Both dataset distillation and data compression fundamentally pursue minimum-entropy representations to eliminate information redundancy.</p>
            </div>
        </section>
        <!-- Method Section -->
        <section class="section" id="method">
            <h2 class="section-title">3. Methodology</h2>
            <div class="section-content">
                <h3 style="color: var(--primary-color); margin: 1.5rem 0 1rem 0;">3.1. Problem Formulation</h3>
                <p>Dataset distillation can be modeled as a bi-level optimization problem. Given an original dataset $T := (X_T, Y_T) = \{(x_i, y_i)\}_{i=1}^N$ consisting of $N$ data points $x_i$ and corresponding labels $y_i$, the goal of dataset distillation is to find a smaller, synthetic dataset $S := (X_S, Y_S) = \{(\tilde{x}_j, \tilde{y}_j)\}_{j=1}^M$, and $M \ll N$, such that a model trained on $S$ performs comparably to a model trained on $T$.</p>
                <p>More formally, let $f(x; \theta)$ represent a model parameterized by $\theta$. The objective of dataset distillation is formulated as follows:</p>
                <div style="text-align: center; margin: 2rem 0;">
                    $$\min_S \mathbb{E}_{(x,y) \sim p(x,y)}[\ell(f_{\theta_S}(x), y)]$$
                    $$\text{where } \theta_S = \arg\min_\theta \frac{1}{M} \sum_{(\tilde{x}_j, \tilde{y}_j) \in S} \ell(f_\theta(\tilde{x}_j), \tilde{y}_j)$$
                </div>
                <h3 style="color: var(--primary-color); margin: 1.5rem 0 1rem 0;">3.2. Hybrid Parameterization Framework</h3>
                <p>We propose a hybrid framework that combines explicit and implicit parameterization by jointly learning both the latent feature $Z$ and transformation function $F_\psi$, while storing parameters of both components. The total rate consists of explicit and implicit components:</p>
                <div style="text-align: center; margin: 2rem 0;">
                    $$R_{total} = R_{ex} + R_{im}$$
                </div>
                <p>where $R_{ex}$ and $R_{im}$ represent the explicit part and implicit part of the rate, respectively.</p>
                <div class="figure">
                    <img src="data/img/figure2.svg" alt="Framework comparison">
                    <figcaption class="figure-caption">Figure 2. Comparison of dataset distillation frameworks. (a) Explicit parameterization. (b) Implicit parameterization. (c) Our framework combines explicit and implicit parametrization and generates compact representations by directly optimizing bitrate of synthetic datasets.</figcaption>
                </div>
            </div>
        </section>
        <!-- Experiments Section -->
        <section class="section" id="experiments">
            <h2 class="section-title">4. Experiments</h2>
            <div class="section-content">
                <p>We conduct comprehensive experiments on CIFAR-10, CIFAR-100, and ImageNet-128 to demonstrate the effectiveness of our joint rate-utility optimization approach. Our method consistently achieves superior rate-utility trade-offs compared to existing dataset distillation methods.</p>
                <h3 style="color: var(--primary-color); margin: 1.5rem 0 1rem 0;">4.1. Experimental Setup</h3>
                <p>We evaluate our method across diverse bpc budgets, distillation losses, and backbone architectures. The experiments are designed to validate the generalizability and effectiveness of our approach under various conditions.</p>
                <h3 style="color: var(--primary-color); margin: 1.5rem 0 1rem 0;">4.2. Results</h3>
                <p>Our method achieves up to 170√ó greater compression than standard distillation at comparable accuracy. The results demonstrate that our joint optimization approach consistently establishes new state-of-the-art rate-utility trade-offs across all tested datasets and configurations.</p>
                <div class="figure">
                    <img src="data/img/figure3.svg" alt="Experimental results">
                    <figcaption class="figure-caption">Figure 3. Experimental results showing the superior performance of our method across different datasets and compression ratios.</figcaption>
                </div>
            </div>
        </section>
        <!-- Conclusion Section -->
        <section class="section" id="conclusion">
            <h2 class="section-title">5. Conclusion</h2>
            <div class="section-content">
                <p>In this work, we have presented a novel joint rate-utility optimization method for dataset distillation that addresses the fundamental limitation of existing approaches that optimize rate and utility separately. Our hybrid parameterization framework successfully combines the advantages of both explicit and implicit representations while enabling end-to-end optimization of the rate-utility trade-off.</p>
                <p>The introduction of the bits per class (bpc) metric provides a standardized and fair way to compare different dataset distillation methods, accounting for all storage costs including samples, labels, and decoder parameters. Our comprehensive experimental evaluation demonstrates that the proposed method consistently achieves superior rate-utility trade-offs across multiple datasets and configurations.</p>
                <p>Future work could explore the application of our framework to other domains beyond image classification, such as natural language processing and speech recognition. Additionally, investigating more sophisticated entropy estimation techniques and exploring the integration with other compression paradigms could further improve the rate-utility trade-offs.</p>
            </div>
        </section>
        <!-- BibTeX Citation Box -->
<div class="bibtex-box" style="margin: 2rem 0; padding: 1rem; background: #f8f9fa; border-radius: 8px; border: 1px solid #e0e0e0;">
    <label for="bibtex-citation" style="font-weight: bold; display: block; margin-bottom: 0.5rem;">BibTeX Citation</label>
    <textarea id="bibtex-citation" style="width: 100%; min-height: 120px; font-family: 'Fira Mono', 'Consolas', monospace; font-size: 0.95rem; padding: 0.5rem; border-radius: 4px; border: 1px solid #ccc; background: #fff; resize: vertical;" readonly>
@article{bao2025ruo,
  author    = {Youneng Bao and Yiping Liu and Zhuo Chen and Yongsheng Liang and Mu Li and Kede Ma},
  title     = {Dataset Distillation as Data Compression: A Rate-Utility Perspective},
  journal   = {ICCV},
  year      = {2025},
}
    </textarea>
    <small style="color: #888;">Copy and paste this BibTeX entry to cite our paper.</small>
</div>
        <!-- References Section -->
        <section class="references" id="references">
            <h2 class="section-title">References</h2>
            <div class="section-content">
                <div class="reference-item">
                    <strong>[1]</strong> Wang, T., Zhu, J., Torralba, A., & Efros, A. A. (2018). Dataset distillation. <em>arXiv preprint arXiv:1811.10959</em>.
                </div>
                <div class="reference-item">
                    <strong>[2]</strong> Ball√©, J., Laparra, V., & Simoncelli, E. P. (2017). End-to-end optimized image compression. <em>ICLR</em>.
                </div>
                <div class="reference-item">
                    <strong>[3]</strong> Zhao, B., Mopuri, K. R., & Bilen, H. (2020). Dataset condensation with gradient matching. <em>ICLR</em>.
                </div>
                <div class="reference-item">
                    <strong>[4]</strong> Kaplan, J., McCandlish, S., Henighan, T., Brown, T. B., Chess, B., Child, R., ... & Amodei, D. (2020). Scaling laws for neural language models. <em>arXiv preprint arXiv:2001.08361</em>.
                </div>
                <div class="reference-item">
                    <strong>[5]</strong> Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., ... & Amodei, D. (2020). Language models are few-shot learners. <em>NeurIPS</em>.
                </div>
            </div>
        </section>
    </main>
    <!-- Footer -->
    <footer class="footer">
        <div class="footer-content">
            <h3>Dataset Distillation as Data Compression: A Rate-Utility Perspective</h3>
            <p><strong>Authors:</strong> Youneng Bao, Yiping Liu, Zhuo Chen, Yongsheng Liang, Mu Li, Kede Ma</p>
            <p><strong>Conference:</strong> ICCV 2025</p>
            <p><strong>Institutions:</strong> Harbin Institute of Technology, Shenzhen | Peng Cheng Laboratory | City University of Hong Kong</p>
            <p style="margin-top: 1rem; font-size: 0.9rem;">¬© 2025 - This paper is open access under the Creative Commons Attribution License.</p>
        </div>
    </footer>
    <script src="data/main.js"></script>
</body>
</html>

